{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "open_ended_df = pd.read_json(\"datasets/qa_test_4_open_ended.jsonl\", lines=True)\n",
    "open_ended_df[\"etr_predicted_conclusion\"] = open_ended_df[\"scoring_guide\"].apply(lambda x: x['etr_predicted'])\n",
    "open_ended_df[\"categorical\"] = open_ended_df[\"scoring_guide\"].apply(lambda x: x['etr_predicted_conclusion_is_categorical']).astype(bool)\n",
    "open_ended_df[\"consistent\"] = open_ended_df[\"scoring_guide\"].apply(lambda x: x['etr_predicted_is_classically_correct']).astype(bool)\n",
    "open_ended_df[\"premises\"] = open_ended_df[\"scoring_guide\"].apply(lambda x: x[\"generation_details\"]['premises_etr'])\n",
    "\n",
    "yes_no_df = pd.read_json(\"datasets/qa_test_4_yes_no.jsonl\", lines=True)\n",
    "yes_no_df[\"premises\"] = yes_no_df[\"scoring_guide\"].apply(lambda x: x[\"generation_details\"]['premises_etr'])\n",
    "yes_no_df[\"etr_predicted_conclusion\"] = yes_no_df[\"scoring_guide\"].apply(lambda x: x['etr_predicted'])\n",
    "yes_no_df[\"query\"] = yes_no_df[\"scoring_guide\"].apply(lambda x: x[\"yes_no\"][\"conclusion_etr\"])\n",
    "yes_no_df[\"etr_predicted\"] = yes_no_df[\"scoring_guide\"].apply(lambda x: x[\"yes_no\"][\"conclusion_is_etr_predicted\"])\n",
    "yes_no_df[\"classically_correct\"] = yes_no_df[\"scoring_guide\"].apply(lambda x: x[\"yes_no\"][\"conclusion_is_classically_correct\"])\n",
    "\n",
    "def print_open_ended_question(df, categorical: bool, consistent: bool):\n",
    "    try:\n",
    "        question = df[(df[\"categorical\"] == categorical) & (df[\"consistent\"] == consistent)].sample(1)\n",
    "    except ValueError:\n",
    "        print(\"No questions found with categorical={} and consistent={}\".format(categorical, consistent))\n",
    "        return\n",
    "    print(f\"QUESTION ID: {question.index.values[0]}\")\n",
    "    print(\"\")\n",
    "    print(\"PROMPT\")\n",
    "    print(question[\"question\"].values[0])\n",
    "    print(\"\")\n",
    "    print(\"PREMISES\")\n",
    "    for premise in question[\"premises\"].values[0]:\n",
    "        print(premise)\n",
    "    print(\"\")\n",
    "    print(\"ETR PREDICTED CONCLUSION\")\n",
    "    print(question[\"etr_predicted_conclusion\"].values[0])\n",
    "    print(\"\")\n",
    "    print(\"CONCLUSION IS CATEGORICAL\")\n",
    "    print(question[\"categorical\"].values[0])\n",
    "    print(\"\")\n",
    "    print(\"CONCLUSION IS CONSISTENT\")\n",
    "    print(question[\"consistent\"].values[0])\n",
    "\n",
    "def print_does_it_follow_question(df, etr_predicted: bool, classically_correct: bool):\n",
    "    try:\n",
    "        question = df[(df[\"etr_predicted\"] == etr_predicted) & (df[\"classically_correct\"] == classically_correct)].sample(1)\n",
    "    except ValueError:\n",
    "        print(\"No questions found with etr_predicted={} and classically_correct={}\".format(etr_predicted, classically_correct))\n",
    "        return\n",
    "    print(f\"QUESTION ID: {question.index.values[0]}\")\n",
    "    print(\"\")\n",
    "    print(\"PROMPT\")\n",
    "    print(question[\"question\"].values[0])\n",
    "    print(\"\")\n",
    "    print(\"PREMISES\")\n",
    "    for premise in question[\"premises\"].values[0]:\n",
    "        print(premise)\n",
    "    print(\"\")\n",
    "    print(\"QUERY\")\n",
    "    print(question[\"query\"].values[0])\n",
    "    print(\"\")\n",
    "    print(\"QUERY IS ETR PREDICTED\")\n",
    "    print(question[\"etr_predicted\"].values[0])\n",
    "    print(\"\")\n",
    "    print(\"QUERY IS CLASSICALLY CORRECT\")\n",
    "    print(question[\"classically_correct\"].values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-Ended Questions\n",
    "For these questions, we list some number of premises, then ask \"What, if anything,\n",
    "follows?\"\n",
    "\n",
    "Questions can belong to one of four categories:\n",
    "1. ETR's default inference procedure predicts a *categorical result* (\"something that\n",
    "   follows\") and that result is *logically consistent* with the premises.\n",
    "2. ETR's default inference procedure predicts a *categorical result* (\"something that\n",
    "   follows\") and that result is *inconsistent* with the premises.\n",
    "3. ETR's default inference procedure predicts something *non-categorical* (i.e., nothing\n",
    "   directly follows), and that result is *logically consistent* with the premises.\n",
    "4. ETR's default inference procedure predicts something *non-categorical* (i.e., nothing\n",
    "   directly follows), and that result is *inconsistent* with the premises.\n",
    "\n",
    "Case 2. represents the interesting illusory inferences studied by Philip Johnson-Laird\n",
    "and formalized with ETR in *Reason and Inquiry*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION ID: 9\n",
      "\n",
      "PROMPT\n",
      "I'm a dimensional cartographer mapping regions of parallel universes. I need to understand their properties through logical analysis. Here's what I've mapped:\n",
      "\n",
      "* for all A, if A is reality-bending, then A is light-absorbing\n",
      "* for all A, A is reality-bending\n",
      "\n",
      "For the purpose of this question, I want you to write what follows in English. Please be succinct, precise and clear in your answer. Write a logical statement of the form \"From the premises, we can conclude that ...\" and then clearly write your conclusion. Please be succinct, precise, and clear.\n",
      "\n",
      "What if anything follows?\n",
      "\n",
      "I want you to answer immediately. Do not think about it at all, just immediately answer.\n",
      "\n",
      "What follows? Answer in the format that I showed you. Write \"Answer: {logical statement}\".\n",
      "\n",
      "PREMISES\n",
      "∀a {lightAbsorbing(a*)}^{realityBending(a*)}\n",
      "∀a {realityBending(a*)}\n",
      "\n",
      "ETR PREDICTED CONCLUSION\n",
      "∀a {lightAbsorbing(a*)}\n",
      "\n",
      "CONCLUSION IS CATEGORICAL\n",
      "True\n",
      "\n",
      "CONCLUSION IS CONSISTENT\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print_open_ended_question(open_ended_df, categorical=True, consistent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION ID: 5\n",
      "\n",
      "PROMPT\n",
      "I'm an astronomer studying newly discovered celestial bodies. I've made some observations and I need to use logical reasoning to figure out what's going on. Here's what I know so far:\n",
      "\n",
      "* there is some X such that X is elliptically-orbiting and X is orbited by satellites\n",
      "* planet Z is elliptically-orbiting\n",
      "\n",
      "For the purpose of this question, I want you to write what follows in English. Please be succinct, precise and clear in your answer. Write a logical statement of the form \"From the premises, we can conclude that ...\" and then clearly write your conclusion. Please be succinct, precise, and clear.\n",
      "\n",
      "What if anything follows?\n",
      "\n",
      "I want you to answer immediately. Do not think about it at all, just immediately answer.\n",
      "\n",
      "What follows? Answer in the format that I showed you. Write \"Answer: {logical statement}\".\n",
      "\n",
      "PREMISES\n",
      "∃x {orbitedBySatellites(x)ellipticallyOrbiting(x*)}\n",
      "{ellipticallyOrbiting(planetZ()*)}\n",
      "\n",
      "ETR PREDICTED CONCLUSION\n",
      "{orbitedBySatellites(planetZ())}\n",
      "\n",
      "CONCLUSION IS CATEGORICAL\n",
      "True\n",
      "\n",
      "CONCLUSION IS CONSISTENT\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print_open_ended_question(open_ended_df, categorical=True, consistent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION ID: 11\n",
      "\n",
      "PROMPT\n",
      "I'm an alchemist studying mysterious substances in my laboratory. I need to understand their properties through logical reasoning. Here's what I've discovered:\n",
      "\n",
      "* Time Crystal is consciousness-expanding\n",
      "* Time Crystal is spirit-affecting\n",
      "\n",
      "For the purpose of this question, I want you to write what follows in English. Please be succinct, precise and clear in your answer. Write a logical statement of the form \"From the premises, we can conclude that ...\" and then clearly write your conclusion. Please be succinct, precise, and clear.\n",
      "\n",
      "What if anything follows?\n",
      "\n",
      "I want you to answer immediately. Do not think about it at all, just immediately answer.\n",
      "\n",
      "What follows? Answer in the format that I showed you. Write \"Answer: {logical statement}\".\n",
      "\n",
      "PREMISES\n",
      "{consciousnessExpanding(timeCrystal())}\n",
      "{spiritAffecting(timeCrystal())}\n",
      "\n",
      "ETR PREDICTED CONCLUSION\n",
      "{0}\n",
      "\n",
      "CONCLUSION IS CATEGORICAL\n",
      "False\n",
      "\n",
      "CONCLUSION IS CONSISTENT\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print_open_ended_question(open_ended_df, categorical=False, consistent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No questions found with categorical=False and consistent=False\n"
     ]
    }
   ],
   "source": [
    "print_open_ended_question(open_ended_df, categorical=False, consistent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does-it-follow Questions\n",
    "For these questions, we list some number of premises, a conclusion, then ask \"Does this\n",
    "conclusion follow?\"\n",
    "\n",
    "Questions can belong to one of four categories:\n",
    "1. The conclusion *does* follow from ETR and is *logically consistent* with the premises.\n",
    "2. The conclusion *does* follow from ETR and is *inconsistent* with the premises.\n",
    "3. The conclusion does *not* follow from ETR and is *logically consistent* with the premises.\n",
    "4. The conclusion does *not* follow from ETR and is *inconsistent* with the premises.\n",
    "\n",
    "Case 2. represents endorsing an ETR fallacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION ID: 4\n",
      "\n",
      "PROMPT\n",
      "I'm playing a card game against the computer. It's an unusual game with an unusual deck of cards. I have some clues about what's going on, and I need to figure some more things out through logical reasoning. Here's what I know so far:\n",
      "\n",
      "* if the jack is castable, then the jack is square\n",
      "* the jack is castable\n",
      "\n",
      "Does the following conclusion necessarily follow from the given statements?\n",
      "\n",
      "My Conclusion: the jack is castable or the jack is square\n",
      "\n",
      "I want you to answer immediately. Do not think about it at all, just immediately answer.\n",
      "\n",
      "Does it follow? Answer in the form of \"Answer: Yes\" or \"Answer: No\".\n",
      "\n",
      "PREMISES\n",
      "{square(theJack())}^{castable(theJack())}\n",
      "{castable(theJack())}\n",
      "\n",
      "QUERY\n",
      "{castable(theJack()),square(theJack())}\n",
      "\n",
      "QUERY IS ETR PREDICTED\n",
      "True\n",
      "\n",
      "QUERY IS CLASSICALLY CORRECT\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print_does_it_follow_question(yes_no_df, etr_predicted=True, classically_correct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No questions found with etr_predicted=True and classically_correct=False\n"
     ]
    }
   ],
   "source": [
    "print_does_it_follow_question(yes_no_df, etr_predicted=True, classically_correct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION ID: 0\n",
      "\n",
      "PROMPT\n",
      "I'm a synthetic biology researcher studying advanced bioengineered life forms. I need to understand their capabilities through logical analysis. Here's what we've created:\n",
      "\n",
      "* biomatrix is consciousness-developing\n",
      "* for all A, A is time-manipulating\n",
      "\n",
      "Does the following conclusion necessarily follow from the given statements?\n",
      "\n",
      "My Conclusion: biomatrix is time-manipulating and biomatrix is consciousness-developing\n",
      "\n",
      "I want you to answer immediately. Do not think about it at all, just immediately answer.\n",
      "\n",
      "Does it follow? Answer in the form of \"Answer: Yes\" or \"Answer: No\".\n",
      "\n",
      "PREMISES\n",
      "{consciousnessDeveloping(biomatrix()*)}\n",
      "∀biomatrix {timeManipulating(biomatrix*)}\n",
      "\n",
      "QUERY\n",
      "{timeManipulating(biomatrix())consciousnessDeveloping(biomatrix())}\n",
      "\n",
      "QUERY IS ETR PREDICTED\n",
      "False\n",
      "\n",
      "QUERY IS CLASSICALLY CORRECT\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print_does_it_follow_question(yes_no_df, etr_predicted=False, classically_correct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION ID: 3\n",
      "\n",
      "PROMPT\n",
      "I'm working in a materials science lab and we've gotten some puzzling results. I need to use logical reasoning to figure out what's going on. Here's what I know so far:\n",
      "\n",
      "* luminite is not electrically insulating and luminite is not radioactive\n",
      "* luminite is not electrically insulating\n",
      "\n",
      "Does the following conclusion necessarily follow from the given statements?\n",
      "\n",
      "My Conclusion: luminite is electrically insulating\n",
      "\n",
      "I want you to answer immediately. Do not think about it at all, just immediately answer.\n",
      "\n",
      "Does it follow? Answer in the form of \"Answer: Yes\" or \"Answer: No\".\n",
      "\n",
      "PREMISES\n",
      "{~radioactive(luminite())~electricallyInsulating(luminite())}\n",
      "{~electricallyInsulating(luminite())}\n",
      "\n",
      "QUERY\n",
      "{electricallyInsulating(luminite())}\n",
      "\n",
      "QUERY IS ETR PREDICTED\n",
      "False\n",
      "\n",
      "QUERY IS CLASSICALLY CORRECT\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print_does_it_follow_question(yes_no_df, etr_predicted=False, classically_correct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
