

1. Create example_data.jsonl with actual questions
2. Point vincents_task.yaml to the jsonl file location
3. Run the `lm_eval` command with vincents_task as the task
4. Use the --output_path flag with lm_eval to tell it where to put the results
5. Look in the resulting 'samples' jsonl file to see your results, under the 'resps' key

lm_eval --model openai-completions \
    --model_args model=davinci \
    --tasks my/path/to/vincents_task \
    --log_samples \
    --output_path my_results/my_dir \
    --include_path my/path/to/this/project \
    --num_fewshot 0 \
    --verbosity DEBUG

Note the 'include_path' should point to a directory that contains your yaml file, although that can be nested, I think. I think this might also be where the jsonl file is relative to?

You may also need to run `export PYTHONPATH=$PYTHONPATH:/path/to/lm-evaluation-harness` before running the command.

Let me know if you run into issues.
